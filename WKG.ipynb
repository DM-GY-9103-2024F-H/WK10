{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week G\n",
    "\n",
    "ML review, Ingredients for ML models, Intro to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "We've seen a couple of different methods of doing classic machine learning for classification, regression and clustering tasks.\n",
    "\n",
    "Despite operating on different types of data, using different algorithms and serving different purposes, all of the methods we've seen follow a common set of operations, or, pipeline:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/ML_00.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data**: we start with a collection of files or numbers that we process, analyze, visualize and study to understand their content and relationships. We then split this data into $2$ separate datasets, one for training our algorithm and another to test how it performs omn data it hasn't seen.\n",
    "\n",
    "- **Algorithm**: these are the mathematical operations that get performed on the data to extract patterns and relationships between our data points. The algorithm chosen depends on the type of task we are trying to accomplish.\n",
    "\n",
    "- **Cost Function**: in order for the algorithm to learn anything we have to guide it by telling it how close it gets to correct answers. This is particularly important for supervised learning, where the algorithm gets the data to operate on and the correct answer for the task. Some algorithms have built-in cost functions, others take a cost function as a parameters, but either way, this is the function that the algorithm uses to adjust its parameters.\n",
    "\n",
    "- **Evaluation Function**: once our algorithm builds a model from the training data, the evaluation function is what we use to measure how well the model performs on the test dataset. The evaluation function can be the same as the cost function, but is usually a little bit more legible. Where the cost function can be a complex mix of formulas, each meant to guide the algorithm navigate tradeoffs when picking parameters, the evaluation function is meant to validate whether our choice of algorithm, cost function and data is sufficient for our overall goals.\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/ML_01.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of each of these components:\n",
    "- **Data Processing**:\n",
    "  - [Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "  - [Scaling](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "  - [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "- **Cost Function**:\n",
    "  - [Distance](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html)\n",
    "  - [MSE](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
    "  - [Class Likelihood](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html)\n",
    "\n",
    "- **Algorithm**:\n",
    "  - [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "  - [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)\n",
    "  - [SVMs](https://scikit-learn.org/stable/modules/svm.html)\n",
    "  - [Clustering](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "- **Evaluation Function**:\n",
    "  - [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "  - [Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "  - [Other metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/ML_02.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "We've seen this a few times now, and it was usually in the form of a function called `fit()`.\n",
    "\n",
    "Training, or fitting, is the process by which the algorithm combines our data and our cost function to produce a model.\n",
    "\n",
    "The process can be summarized like this:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/training_00.jpg\" width=800px />\n",
    "\n",
    "Every model prediction is compared to the correct output value available in the training data, and the difference between prediction and true value is used to adjust the model's mathematical parameters.\n",
    "\n",
    "If we were to visualize this process over time for a linear regression model and a clustering model, we might see something like this, where the model's performance improves as it uses more data to adjust its parameters:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/training_01.jpg\" width=800px />\n",
    "<br>\n",
    "<img src=\"./imgs/training_02.jpg\" width=800px />\n",
    "\n",
    "\n",
    "Some of the models we've seen so far have \"_closed-form_\" solutions. This means that they're able to look at all of the training data at the same time and, using basic algebraic operations $(+$, $-$, $\\times$, $\\div)$ and matrix algebra, come up with optimal parameters almost instantaneously. This is the case for `PCA` and `Linear Regression` models.\n",
    "\n",
    "This is both a strength and a weakness of these types of models. They train really fast and work really well, as long as we're working with not-so-large datasets for specific tasks. As the variation in our data becomes too large, and the relations we are trying to model grow in complexity, these models start to fall short in quality, or become impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "This has been the most popular method for creating models that can handle very diverse types of data, while not requiring a lot of customization and manual parameter-tuning.\n",
    "\n",
    "The process for training a neural network is exactly the same:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_00.jpg\" width=800px />\n",
    "\n",
    "Except, instead of using very specific matrix operations and algebraic expressions $(+$, $-$, $\\times$, $\\div)$, all of the calculations (processing, predictions, etc) are done using A LOT of very generic, very simple, computational elements called \"_neurons_\" or \"_perceptrons_\":\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_01.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "These are supposed to mimic how actual brain neurons work: they fire and propagate signals depending on the combination and strength of signals present at their inputs.\n",
    "\n",
    "The operation of a single neuron is quite simple:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_02.jpg\" width=800px />\n",
    "\n",
    "They first perform a weighted sum of their input signals:\n",
    "\n",
    "$\\displaystyle Z = w_A \\cdot A + w_B \\cdot B + w_C \\cdot C + ... $\n",
    "\n",
    "and then, an _activation function_ determines if and how the neuron fires, based on this weighted sum and a threshold value.\n",
    "\n",
    "This looks a lot like the function that we try to optimize during linear regression:\n",
    "\n",
    "$\\displaystyle Y = \\beta_0 \\cdot x_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + ... $\n",
    "\n",
    "The difference is that before we had $N$ parameters for $N$ features, and with a neural network we'll have around $N$ parameters per node and about $N$ nodes (one for each feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks\n",
    "\n",
    "Despite this increase in the number of parameters, neural networks are beneficial because they are modular, and relatively simple to extend when needed.\n",
    "\n",
    "Where previously we needed to know very specific strategies for dealing with different types of problems (adding non-linear functions to linear regression, or dimensionality reduction with PCA, or normalization), with neural networks, we can just add more layers and more nodes:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_03.jpg\" width=800px />\n",
    "\n",
    "Another benefit of neural networks is that the same architecture can be used for different tasks by just changing the activation function of the nodes in the last layers.\n",
    "\n",
    "For regression tasks we remove any activation function and just use the node's weighted sum. For classification tasks, we use something called a `softmax()` function that turns the weighted sum into a likelihood value:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/softmax_00.jpg\" width=800px />\n",
    "\n",
    "Other tasks, like image detection, segmentation, generation or encoding, just use a combination of these two types of output nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Networks\n",
    "\n",
    "Training neural networks is a process similar to the `fit()` step we've seen in other models:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_00.jpg\" width=800px />\n",
    "\n",
    "But, because they tend to have A LOT more parameters, training takes A LOT more data and A LOT more time.\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_04.jpg\" width=800px />\n",
    "\n",
    "The cost function is still responsible for starting the process of adjusting the parameters in a neural network, using a method called _Backpropagation_.\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_05.jpg\" width=800px />\n",
    "\n",
    "For every record (or batch of records) in the training dataset, the cost function informs each node how their parameters could be changed in order to decrease the error on the output:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_06.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bit More Detail\n",
    "\n",
    "<img src=\"./imgs/slides_00.jpg\" width=800px />\n",
    "\n",
    "<a href=\"https://docs.google.com/presentation/d/1ppf-nxKS9QKvuNrx37SVkiAs4nk8qJv9JZz7WhhwjFo/\">SLIDES</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "We'll be using the [PyTorch](https://pytorch.org/) library for working with Neural Networks.\n",
    "\n",
    "Before we start building, training, tuning models, we have to learn a little bit about [Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)!\n",
    "\n",
    "Tensor is a fancy word for multi-dimensional list. They are very much like lists, where they keep a sequence of number values, or a sequence of other tensors. They are a little bit more picky than lists because they require all members to be of the same _type_ (all integers, or all floats, etc), and they don't like having inner lists of different lengths.\n",
    "\n",
    "PyTorch tensors are optimized for doing neural network operations, and so they come with a few extra capabilities beyond `sum()`, `sort()`, `mean()`, etc.\n",
    "\n",
    "Let's start by importing them, and taking a look at how to work with multi-dimensional tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import zeros_like, tensor\n",
    "\n",
    "from image_utils import open_image, make_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Shaping\n",
    "\n",
    "Let's open up an image and load its pixels into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg = open_image(\"./data/arara.jpg\")\n",
    "\n",
    "display(mimg)\n",
    "print(mimg.pixels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have to pass the list of pixels to the `tensor()` constructor.\n",
    "\n",
    "We can check it's size with the `shape` member variable, and use slicing and indexing like we've always used with lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_t = tensor(mimg.pixels)\n",
    "mimg_t.shape, mimg_t[:5], mimg_t[5], mimg_t[5][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this tensor is $607,500 \\times 3$, meaning that we have $607,500$ pixels and each pixel has $3$ color values.\n",
    "\n",
    "Let's reshape the tensor so it's more representative of our image's dimensions. We want to have a tensor of shape $h \\times w \\times 3$, where $h$ and $w$ are the images `height` and `width` dimensions.\n",
    "\n",
    "The `reshape()` function does just this, we just have to pass the parameters in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_t = tensor(mimg.pixels).reshape(mimg.size[1], mimg.size[0], 3)\n",
    "\n",
    "mimg_t.shape, mimg_t[:5].shape, mimg_t[:5], mimg_t[0][5], mimg_t[0, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `mimg_t[:5]` doesn't refer to first $5$ pixels anymore, but to the first $5$ rows of our image.\n",
    "\n",
    "To get the first $5$ pixels we can use `mimg_t[0][:5]` or `mimg_t[0, :5]`.\n",
    "\n",
    "That's new syntax! using multiple numbers inside the square brackets, separated with a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_t[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "This is where it starts to get fun.\n",
    "\n",
    "Since we can \n",
    "\n",
    "# TODO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0,y0 = 240, 30\n",
    "\n",
    "mimg_crop_t = mimg_t[y0:y0+256, x0:x0+256]\n",
    "\n",
    "mimg_crop_t.shape, mimg_crop_t[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_crop = make_image(mimg_crop_t)\n",
    "display(mimg_crop)\n",
    "mimg_crop.pixels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_crop_r_t = mimg_crop_t.clone()\n",
    "mimg_crop_r_t[:, :, 1:3] = 0\n",
    "\n",
    "mimg_crop_r_t[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_image(mimg_crop_r_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_crop_g_t = mimg_crop_t.clone()\n",
    "mimg_crop_g_t[:, :, 0] = 0\n",
    "mimg_crop_g_t[:, :, 2] = 0\n",
    "\n",
    "mimg_crop_b_t = mimg_crop_t.clone()\n",
    "mimg_crop_b_t[:, :, 0:2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_image(mimg_crop_r_t))\n",
    "display(make_image(mimg_crop_g_t))\n",
    "display(make_image(mimg_crop_b_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_crop_rgb_t = mimg_crop_g_t.clone()\n",
    "\n",
    "mimg_crop_rgb_t[:, 32:] += mimg_crop_r_t[:, :-32]\n",
    "mimg_crop_rgb_t[:, :-32] += mimg_crop_b_t[:, 32:]\n",
    "\n",
    "display(make_image(mimg_crop_rgb_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_image(mimg_crop_t[:,:,0]))\n",
    "display(make_image(mimg_crop_t[:,:,1]))\n",
    "display(make_image(mimg_crop_t[:,:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1]) > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgtg_idx = (mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1]) > 80\n",
    "rgtb_idx = (mimg_crop_t[:,:,0] - mimg_crop_t[:,:,2]) > 80\n",
    "\n",
    "red_idx = rgtg_idx & rgtb_idx\n",
    "not_red_idx = ~red_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_idx_bool_t = mimg_crop_t.clone()\n",
    "mimg_idx_bool_t[not_red_idx] = tensor((0,0,0))\n",
    "\n",
    "display(make_image(mimg_idx_bool_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_idx_bool_t = zeros_like(mimg_crop_t)\n",
    "mimg_idx_bool_t[red_idx] = mimg_crop_t[red_idx]\n",
    "\n",
    "display(make_image(mimg_idx_bool_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
